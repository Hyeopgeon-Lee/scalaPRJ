// íŒ¨í‚¤ì§€ ì„ ì–¸ - ì´ íŒŒì¼ì€ spark íŒ¨í‚¤ì§€ì— í¬í•¨ë¨
package spark

// í•„ìš”í•œ Spark SQL ê´€ë ¨ í´ë˜ìŠ¤ import
import org.apache.spark.sql.{SparkSession, functions => F}

// MaxMind GeoIP2 ë¼ì´ë¸ŒëŸ¬ë¦¬ import (IP â†’ êµ­ê°€ ì •ë³´ ì¡°íšŒìš©)
import com.maxmind.geoip2.DatabaseReader
import java.io.File
import java.net.InetAddress
import scala.util.Try // ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©

// GeoIPCountryAnalysis ê°ì²´ ì •ì˜ - ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì§„ì…ì 
object GeoIPCountryAnalysis {
  def main(args: Array[String]): Unit = {

    // 1. SparkSession ìƒì„±
    //    - Spark SQL, DataFrame ë“±ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì§„ì…ì 
    //    - .master("local[*]")ëŠ” ë¡œì»¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
    val spark = SparkSession.builder()
      .appName("GeoIP Country Analysis")
      .master("local[*]")
      .getOrCreate()

    // ë¶ˆí•„ìš”í•œ ë¡œê·¸ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ë¡œê·¸ ë ˆë²¨ì„ ERRORë¡œ ì„¤ì •
    spark.sparkContext.setLogLevel("ERROR")

    // ìŠ¤íŒŒí¬ SQL í•¨ìˆ˜ ì‚¬ìš© ë° DataFrame to Dataset ë³€í™˜ ìœ„í•´ ì•”ì‹œì  import
    import spark.implicits._

    // 2. GeoIP ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    //    - `resources` í´ë”ì— ìœ„ì¹˜í•œ GeoLite2-Country.mmdb íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜´
    val geoIPFilePath = this.getClass.getResource("/GeoLite2-Country.mmdb").getPath

    // 3. IP ì£¼ì†Œë¥¼ êµ­ê°€ ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
    //    - ê° í˜¸ì¶œ ì‹œë§ˆë‹¤ DatabaseReaderë¥¼ ìƒˆë¡œ ìƒì„± â†’ ë¹„íš¨ìœ¨ì ì´ì§€ë§Œ êµ¬ì¡°ëŠ” ë‹¨ìˆœ
    //    - readerëŠ” thread-safeí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì´ ë°©ì‹ì€ ì•ˆì „
    //    - ì˜ˆì™¸ ë°œìƒ ì‹œ "Unknown" ë°˜í™˜
    def ipToCountry(ip: String): String = {
      Try {
        val dbFile = new File(geoIPFilePath)
        val reader = new DatabaseReader.Builder(dbFile).build()
        val country = reader.country(InetAddress.getByName(ip)).getCountry.getName
        reader.close() // ë¦¬ì†ŒìŠ¤ í•´ì œ
        country
      }.getOrElse("Unknown") // ì˜ˆì™¸ ë°œìƒ ì‹œ fallback
    }

    // 4. ìœ„ í•¨ìˆ˜ë¥¼ UDFë¡œ ë“±ë¡í•˜ì—¬ Spark SQLì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
    val ipToCountryUDF = F.udf(ipToCountry _)

    // 5. Apache ë¡œê·¸ JSON íŒŒì¼ ì½ê¸°
    //    - HDFSì— ì €ì¥ëœ JSON íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë¶ˆëŸ¬ì˜´
    //    - multiline: JSON ë°°ì—´ í˜•íƒœë¼ë©´ true ì„¤ì • í•„ìš”
    val df = spark.read
      .option("multiline", value = true)
      .json("hdfs://192.168.133.131:8020/spark/apache_log.json")

    // 6. êµ­ê°€ ì»¬ëŸ¼ ì¶”ê°€ ë° êµ­ê°€ë³„ ìš”ì²­ ìˆ˜ ì§‘ê³„
    //    - withColumn("country", ...): IP â†’ êµ­ê°€ ë³€í™˜
    //    - groupBy + count: êµ­ê°€ë³„ ìš”ì²­ íšŸìˆ˜ ì§‘ê³„
    //    - orderBy: ìš”ì²­ ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    val resultDF = df
      .withColumn("country", ipToCountryUDF($"ip"))
      .groupBy("country")
      .count()
      .orderBy(F.desc("count"))

    // 7. ìµœì¢… ê²°ê³¼ ì¶œë ¥ (ìµœëŒ€ 100ê°œ, ê°’ ìë¥´ì§€ ì•ŠìŒ)
    println("ğŸ“Š êµ­ê°€ë³„ ìš”ì²­ ìˆ˜:")
    resultDF.show(100, truncate = false)

    // 8. Spark ì„¸ì…˜ ì¢…ë£Œ (ë¦¬ì†ŒìŠ¤ ë°˜í™˜)
    spark.stop()
  }
}
